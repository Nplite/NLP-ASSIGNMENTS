{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc44854",
   "metadata": {},
   "source": [
    "# Assignment 04 Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35eaef1",
   "metadata": {},
   "source": [
    "#### 1.\tCan you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47871a8c",
   "metadata": {},
   "source": [
    "In Sequence to Sequence Learning, RNN is trained to map an input sequence to an output sequence which is not necessarily of the same length. Applications are speech recognition, machine translation, image captioning and question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478edb2",
   "metadata": {},
   "source": [
    "#### 2.\tWhy do people use encoder–decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f97c323",
   "metadata": {},
   "source": [
    "This two-step model, called an Encoder–Decoder, works much better than trying to translate on the fly with a single sequence-to-sequence RNN (like the one represented on the top left), since the last words of a sentence can affect the first words of the translation, so you need to wait until you have heard the whole ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb759d62",
   "metadata": {},
   "source": [
    "#### 3.\tHow could you combine a convolutional neural network with an RNN to classify videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f1a41",
   "metadata": {},
   "source": [
    "1.The images of a video are fed to a CNN model to extract high-level features. The features are then fed to an RNN layer and the output of the RNN layer is connected to a fully connected layer to get the classification output.\n",
    "2.Each video is converted into sequential images and passed onto the CNN to extract spatial features. The outputs are then passed into a recurrent sequence learning model (i.e. LSTM) to identify temporal features within the image sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639b7b3",
   "metadata": {},
   "source": [
    "#### 4.\tWhat are the advantages of building an RNN using dynamic_rnn() rather than static_rnn()?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e4944a0",
   "metadata": {},
   "source": [
    "# 1. It is based on awhile_loop()operation that is able to swap the GPU’s memory to theCPU’s memory during backpropagation, avoiding out-of-memory errors.\n",
    "\n",
    "# 2. It is arguably easier to use, as it can directly take a single tensor as input and output(covering all time steps), rather than a list of tensors (one per time step). No need to stack,unstack, or transpose.\n",
    "\n",
    "#3. It generates a smaller graph, easier to visualize in TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d0589",
   "metadata": {},
   "source": [
    "#### 5.\tHow can you deal with variable-length input sequences? What about variable-length output sequences?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "440e3971",
   "metadata": {},
   "source": [
    "1.The most common way people deal with inputs of varying length is padding.I first define the desired sequence length, i.e. the input length you want your model to have. Then any sequences with a shorter length than this are padded either with zeros or with special characters so that they reach the desired length. If an input is larger than your desired length, usually you'd split it into multiple inputs.\n",
    "2. The first and simplest way of handling variable length input is to set a special mask value in the dataset, and pad out the length of each input to the standard length with this mask value set for all additional entries created. Then, create a Masking layer in the model, placed ahead of all downstream layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa3100",
   "metadata": {},
   "source": [
    "#### 6.\tWhat is a common way to distribute training and execution of a deep RNN across multiple GPUs?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "63b3a1ae",
   "metadata": {},
   "source": [
    "If you want to tackle distributed training with multiple GPUs, it will first be important to recognize whether you will need to use data parallelism or model parallelism. This decision will be based on the size and scope of your datasets. \n",
    "\n",
    "Are you able to have each GPU run the entire model with the dataset? Or will it be more time-efficient to run different portions of the model across multiple GPUs with larger datasets? Generally, Data Parallelism is the standard option for distributed learning. Start with synchronous data parallelism before delving deeper into model parallelism or asynchronous data parallelism where a separate dedicated parameter server is needed.\n",
    "\n",
    "We can begin to link your GPUs together in your distributed training process.\n",
    "\n",
    "Break your data down based on your parallelism decision. For example, you might use the current data batch (the global batch) and divide it across eight sub-batches (local batches). If the global batch has 512 samples and you have eight GPUs, each of the eight local batches will include 64 samples.\n",
    "Each of the eight GPUs, or mini-processors, runs a local batch independently: forward pass, backward pass, output the weights' gradient, etc.\n",
    "Weight modifications from local gradients are efficiently blended across all eight mini-processors so everything stays in sync and the model has trained appropriately (when using synchronous data parallelism).\n",
    "It is important to remember that one GPU for distributed training will need to host the collected data and results of the other GPUs during the training phase. You can run into the issue of one GPU running out of memory if you are not paying close attention.\n",
    "\n",
    "Other than this, the benefits far outweigh the cost when considering distributed training with multiple GPUs! In the end, each GPU reduces time spent in the training phase, increases model efficiency, and yields more high-end results when you choose the correct data parallelization for your model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
